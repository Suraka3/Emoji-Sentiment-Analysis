{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sk2KvwYLc4Pa",
    "outputId": "91a663a9-cd15-4d46-a9b3-fd67c6242d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/lionardo/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lionardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w1xOCOy4Cvqm"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "07MoI2buaYVI",
    "outputId": "56da5589-9ca0-4bf6-f5ea-acad7d121c70",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /anaconda3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: regex in /anaconda3/lib/python3.6/site-packages (2018.11.22)\n",
      "Collecting csv\n",
      "\u001b[31m  Could not find a version that satisfies the requirement csv (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install regex\n",
    "!pip install csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "ymt3TLJvDdVE",
    "outputId": "0a8eeebe-6a6d-4656-8f59-7db674c0e590"
   },
   "outputs": [],
   "source": [
    "importdf=pd.read_csv('emoji_cnt_from_q2_597549_samples.csv')\n",
    "#importdf=pd.read_csv('german_tweets_text_id_only_q2_597549_samples.csv')\n",
    "importdf.dropna(inplace=True)\n",
    "importdf.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoEcP6zGaYVL"
   },
   "outputs": [],
   "source": [
    "tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "emoji_cnt_file = \"emoji_cnt_from_q2_597549_samples.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXOtDdRne1lR"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex\n",
    "import csv\n",
    "\n",
    "\n",
    "def split_count(text):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "            \n",
    "            return emoji_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    return emoji_cnt\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\n",
    "emoji_cnt ={}\n",
    "\n",
    "         \n",
    "       with open(tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "        continue\n",
    "           #print(', '.join(row))\n",
    "            line = [row[1]]\n",
    "        \n",
    "            counter = split_count(line[0])\n",
    "            unique_counter = {}\n",
    "            for emoji in counter:\n",
    "                unique_counter[emoji] = 1\n",
    "            if len(unique_counter) not in emoji_cnt:\n",
    "                emoji_cnt[len(unique_counter)] = 0\n",
    "            emoji_cnt[len(unique_counter)] += 1    \n",
    "   \n",
    "\n",
    "  return emoji_cnt\n",
    "\n",
    "\n",
    "       print(emoji_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119fd9160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "y = (31664,10829, 3886,1133,458,209,117,50,34,15,6,3,3,2)\n",
    "x = (1,2,3,4,5,6,7,8,9,10,11,12,13,14)\n",
    "plt.bar(x,y,align='center') # A bar chart\n",
    "plt.xlabel('count emojes')\n",
    "plt.ylabel('tweets')\n",
    "for i in range(len(y)):\n",
    "    plt.hlines(y[i],0,x[i]) # Here you are drawing the horizontal lines\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUS0nVP-ots8"
   },
   "source": [
    "import altair as alt\n",
    "alt.Chart(count_emojis_in_tweets(tweet_file)).mark_bar().encode(\n",
    "  x=alt.X('Miles_per_Gallon', bin=True),\n",
    "  y='count()',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 387605, 1: 31664, 3: 3886, 2: 10829, 6: 209, 4: 1133, 5: 458, 9: 34, 13: 3, 12: 3, 8: 50, 15: 2, 10: 15, 7: 117, 23: 1, 14: 2, 11: 6, 16: 2, 24: 1, 17: 2, 20: 1, 19: 1}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "def count_emojis_in_tweets(tweet_file):\n",
    "    emoji_cnt = {}\n",
    "    \n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            #print(', '.join(row))\n",
    "            line = [row[1]]\n",
    "        \n",
    "            counter = split_count(line[0])\n",
    "            unique_counter = {}\n",
    "            for emoji in counter:\n",
    "                unique_counter[emoji] = 1\n",
    "            if len(unique_counter) not in emoji_cnt:\n",
    "                emoji_cnt[len(unique_counter)] = 0\n",
    "            emoji_cnt[len(unique_counter)] += 1    \n",
    "    return emoji_cnt\n",
    "\n",
    "#'''            \n",
    "           # i += 1\n",
    "           # if i > 100:\n",
    "             #   break;\n",
    "#'''         \n",
    "\n",
    "tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "emoji_cnt_file = \"emoji_cnt_from_q2_597549_samples.csv.\"\n",
    "print(count_emojis_in_tweets(tweet_file))\n",
    "#plt.hist {0: 387605, 1: 31664, 3: 3886, 2: 10829, 6: 209, 4: 1133, 5: 458, 9: 34, 13: 3, 12: 3, 8: 50, 15: 2, 10: 15, 7: 117, 23: 1, 14: 2, 11: 6, 16: 2, 24: 1, 17: 2, 20: 1, 19: 1}\n",
    "    \n",
    "    #print count\n",
    "    #  s=str(strs, \"unicode\")\n",
    "    #print(s)\n",
    "    #print(strs)\n",
    "   #emoti = re.finditer(r'[\\U0001f600-\\U0001f650]', s)\n",
    "    #count = sum(1 for _ in emoti)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emoji_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-342-9e5b5a69fe21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0memoji_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0memoji_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'emoji_list' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "from collections import Counter\n",
    "    \n",
    "import regex\n",
    "tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "def split_count(tweet_file):\n",
    "\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', tweet_file)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "\n",
    "    return emoji_list\n",
    "\n",
    "print (emoji_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-527-c02e6c529949>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-527-c02e6c529949>\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    combine_emoji = combine_emoji + \"+\" + emojiList[i]\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "from collections import Counter\n",
    "\n",
    "def CountEmojiCom(tweet_file,emoji_amount = 2):\n",
    "    #tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    combine_emoji_dict = {}\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "                # line = [row[1]]\n",
    "        \n",
    "            counter = split_count(line[0])\n",
    "            #unique_counter = {}\n",
    "            for emoji in counter:\n",
    "               # def CountEmojiCom(tex_list,emoji_amount = 2):\n",
    "    #combine_emoji_dict = {}\n",
    "     #[emoji1,emoji2]\n",
    "        #note: find_emoji() I didn't implement, you implement by yourself\n",
    "        \n",
    "        \n",
    "       # combine_emoji = \"\"\n",
    "        #for i in range(emoji_amount):\n",
    "            combine_emoji = combine_emoji + \"+\" + emojiList[i]\n",
    "        # Beispiel: \"herz+simle\"\n",
    "        if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "            combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "        else:\n",
    "            combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    most_emoji_combine = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "    # [\"cry\",\"sad\"]\n",
    "    \n",
    "    return (most_emoji_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-543-63ed70b141d0>, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-543-63ed70b141d0>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    combine_emoji_dict = {}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "from collections import Counter\n",
    "\n",
    "def count_emojis_in_tweets(tweet_file):\n",
    "    emoji_cnt = {}\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "                def CountEmojiCom(text_list,emoji_amount = 2):\n",
    "    combine_emoji_dict = {}\n",
    "    for test in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        \n",
    "        combine_emoji = \"\"\n",
    "        for i in range(emoji_amount):\n",
    "            combine_emoji = combine_emoji + \"+\" + emojiList[i]\n",
    "        # Beispiel: \"herz+simle\"\n",
    "        if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "            combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "        else:\n",
    "            combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    most_emoji_combine = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis = 2):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [row[1]]\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    \n",
    "print(CountMostEmojisCombInTweets())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ˜‚']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ”ž', 'ðŸ“½']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â­', 'â­', 'â–¶', 'â—€']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ’¥', 'ðŸ’¥', 'ðŸ’¥', 'ðŸ’¥', 'ðŸ’¥', 'ðŸ’¥']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(7))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ˜‚']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ˜', 'ðŸ’ªðŸ»', 'ðŸ‘ŠðŸ»', 'ðŸ’¯', 'ðŸ”´', 'âš½ï¸', 'âš½ï¸', 'âš½ï¸', 'âš½ï¸']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(9))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸', 'â­ï¸']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ðŸ‘º', 'ðŸ‘º', 'ðŸ', 'ðŸ', 'ðŸ‘º', 'ðŸ‘º', 'ðŸ‘º', 'ðŸ‘º', 'ðŸ‘º', 'ðŸ‘º', 'ðŸ‘º']\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import emoji\n",
    "import re\n",
    "import io\n",
    "import operator \n",
    "#import json\n",
    "import regex\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)       \n",
    "    return emoji_list\n",
    "\n",
    "def CountEmojiCom(text_list,emoji_amount):\n",
    "    combine_emoji_dict = {}\n",
    "    for text in text_list:\n",
    "    \n",
    "        # Beispiel: I love computer vision [\"herz\" ] ==> [\"herz\"]\n",
    "        emoji_list = split_count(text) #[emoji1,emoji2]    \n",
    "        if len(emoji_list) == emoji_amount:\n",
    "            #print(\"hhh\")\n",
    "            #print(emoji_list[0].encode('utf-8'))\n",
    "            #print(emoji_list[1].encode('ascii','xmlcharrefreplace'))\n",
    "            #print(emoji_list[2].encode('ascii','xmlcharrefreplace'))\n",
    "            combine_emoji = \"\"\n",
    "            for i in range(emoji_amount):\n",
    "                combine_emoji = combine_emoji + emoji_list[i] + \"+\"\n",
    "            combine_emoji = combine_emoji.strip(\"+\")\n",
    "            #print(combine_emoji.encode('ascii','xmlcharrefreplace'))\n",
    "            # Beispiel: \"herz+simle\"\n",
    "            if combine_emoji in set(combine_emoji_dict.keys()):\n",
    "                combine_emoji_dict[combine_emoji] = combine_emoji_dict[combine_emoji] + 1\n",
    "            else:\n",
    "                combine_emoji_dict[combine_emoji] = 0\n",
    "       # combine_emoji = \"\"\n",
    "    \n",
    "    # combine_emoji_dict will look like:\n",
    "    # {\n",
    "    # \"herz+simle\":12\n",
    "    # \"cry+sad\": 13\n",
    "    # \"simle+sad\":1\n",
    "    # }\n",
    "    #for key in combine_emoji_dict.keys():\n",
    "       # print(str(key.encode('utf-8')) + \": \" + str(combine_emoji_dict[key]))\n",
    "    #most_emoji_combine_my = max(combine_emoji_dict.items(), key=lambda x: x[1])[0]\n",
    "    most_emoji_combine = max(combine_emoji_dict,key=combine_emoji_dict.get)\n",
    "    \n",
    "    #print(most_emoji_combine.encode('utf-8'))\n",
    "\n",
    "    # Beispiel: \"cry+sad\"\n",
    "    \n",
    "    most_emoji_list = most_emoji_combine.split(\"+\")\n",
    "   # print(most_emoji_list[0].encode('utf-8'))\n",
    "    # [\"cry\",\"sad\"]\n",
    "    return (most_emoji_list)\n",
    "    \n",
    "def CountMostEmojisCombInTweets(amout_emojis):\n",
    "    tweet_file = \"german_tweets_text_id_only_q2_597549_samples.csv\"\n",
    "    text_list = []\n",
    "    with open (tweet_file, 'r', encoding='utf-8', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            line = [(row[1])]\n",
    "            #print(line[0].encode('ascii','xmlcharrefreplace'))\n",
    "            text_list.append(line[0])\n",
    "    most_emoji_list = CountEmojiCom(text_list, amout_emojis)\n",
    "    return most_emoji_list\n",
    "    \n",
    "print(CountMostEmojisCombInTweets(11))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import data_helpers\n",
    "from w2v import train_word2vec\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(0)\n",
    "\n",
    "# ---------------------- Parameters section -------------------\n",
    "#\n",
    "# Model type. See Kim Yoon's Convolutional Neural Networks for Sentence Classification, Section 3\n",
    "model_type = \"CNN-non-static\"  # CNN-rand|CNN-non-static|CNN-static\n",
    "\n",
    "# Data source\n",
    "data_source = \"keras_data_set\"  # keras_data_set|local_dir\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 50\n",
    "filter_sizes = (3, 8)\n",
    "num_filters = 10\n",
    "dropout_prob = (0.5, 0.8)\n",
    "hidden_dims = 50\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepossessing parameters\n",
    "sequence_length = 400\n",
    "max_words = 5000\n",
    "\n",
    "# Word2Vec parameters (see train_word2vec)\n",
    "min_word_count = 1\n",
    "context = 10\n",
    "\n",
    "#\n",
    "# ---------------------- Parameters end -----------------------\n",
    "\n",
    "\n",
    "def load_data(data_source):\n",
    "    assert data_source in [\"keras_data_set\", \"local_dir\"], \"Unknown data source\"\n",
    "    if data_source == \"keras_data_set\":\n",
    "        (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words, start_char=None,\n",
    "                                                              oov_char=None, index_from=None)\n",
    "\n",
    "        x_train = sequence.pad_sequences(x_train, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "        x_test = sequence.pad_sequences(x_test, maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "        vocabulary = imdb.get_word_index()\n",
    "        vocabulary_inv = dict((v, k) for k, v in vocabulary.items())\n",
    "        vocabulary_inv[0] = \"<PAD/>\"\n",
    "    else:\n",
    "        x, y, vocabulary, vocabulary_inv_list = data_helpers.load_data()\n",
    "        vocabulary_inv = {key: value for key, value in enumerate(vocabulary_inv_list)}\n",
    "        y = y.argmax(axis=1)\n",
    "\n",
    "        # Shuffle data\n",
    "        shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "        x = x[shuffle_indices]\n",
    "        y = y[shuffle_indices]\n",
    "        train_len = int(len(x) * 0.9)\n",
    "        x_train = x[:train_len]\n",
    "        y_train = y[:train_len]\n",
    "        x_test = x[train_len:]\n",
    "        y_test = y[train_len:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, vocabulary_inv\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "print(\"Load data...\")\n",
    "x_train, y_train, x_test, y_test, vocabulary_inv = load_data(data_source)\n",
    "\n",
    "if sequence_length != x_test.shape[1]:\n",
    "    print(\"Adjusting sequence length for actual size\")\n",
    "    sequence_length = x_test.shape[1]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocabulary_inv)))\n",
    "\n",
    "# Prepare embedding layer weights and convert inputs for static model\n",
    "print(\"Model type is\", model_type)\n",
    "if model_type in [\"CNN-non-static\", \"CNN-static\"]:\n",
    "    embedding_weights = train_word2vec(np.vstack((x_train, x_test)), vocabulary_inv, num_features=embedding_dim,\n",
    "                                       min_word_count=min_word_count, context=context)\n",
    "    if model_type == \"CNN-static\":\n",
    "        x_train = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_train])\n",
    "        x_test = np.stack([np.stack([embedding_weights[word] for word in sentence]) for sentence in x_test])\n",
    "        print(\"x_train static shape:\", x_train.shape)\n",
    "        print(\"x_test static shape:\", x_test.shape)\n",
    "\n",
    "elif model_type == \"CNN-rand\":\n",
    "    embedding_weights = None\n",
    "else:\n",
    "    raise ValueError(\"Unknown model type\")\n",
    "\n",
    "# Build model\n",
    "if model_type == \"CNN-static\":\n",
    "    input_shape = (sequence_length, embedding_dim)\n",
    "else:\n",
    "    input_shape = (sequence_length,)\n",
    "\n",
    "model_input = Input(shape=input_shape)\n",
    "\n",
    "# Static model does not have embedding layer\n",
    "if model_type == \"CNN-static\":\n",
    "    z = model_input\n",
    "else:\n",
    "    z = Embedding(len(vocabulary_inv), embedding_dim, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "\n",
    "z = Dropout(dropout_prob[0])(z)\n",
    "\n",
    "# Convolutional block\n",
    "conv_blocks = []\n",
    "for sz in filter_sizes:\n",
    "    conv = Convolution1D(filters=num_filters,\n",
    "                         kernel_size=sz,\n",
    "                         padding=\"valid\",\n",
    "                         activation=\"relu\",\n",
    "                         strides=1)(z)\n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Flatten()(conv)\n",
    "    conv_blocks.append(conv)\n",
    "z = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "z = Dropout(dropout_prob[1])(z)\n",
    "z = Dense(hidden_dims, activation=\"relu\")(z)\n",
    "model_output = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Initialize weights with word2vec\n",
    "if model_type == \"CNN-non-static\":\n",
    "    weights = np.array([v for v in embedding_weights.values()])\n",
    "    print(\"Initializing embedding layer with word2vec weights, shape\", weights.shape)\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([weights])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=num_epochs,\n",
    "          validation_data=(x_test, y_test), verbose=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "emoji_statistics.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
